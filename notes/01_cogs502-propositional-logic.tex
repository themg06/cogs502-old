\documentclass[11pt]{article}

\usepackage{natbib,unatbib}
\usepackage[nohide,twocolumn]{ulecnot}
\usepackage{xyling}

\usepackage{ulogicproof}

\pagestyle{fancy}
\lhead{COGS 502 -- Programming and Logic}
\chead{Propositional Logic}
\rhead{Updated \it \today}
\lfoot{Umut \"Ozge}
\cfoot{}
\rfoot{Page \thepage/\pageref{LastPage}}
\setlength{\headheight}{13.6pt}

\begin{document}

\section{Logic, language, thought}
\begin{itemize}
\item Be advised not to take logic as a model of human thought and reasoning.
\item Thought and reasoning are fairly complicated mental processes that do not
seem to behave as dictated -- at least -- by standard logics, and are the
subject of psychology, linguistics, artificial intelligence and
related fields. 

\item Laws of reasoning in logic, with some provisos, may be argued to apply to the end-products of thought,
idealized as propositions, predicates, quantification, and so on, which are
expressed in a formal (for now read as ``human-made'' or ``artificial'') language.
Logical laws can at best be utilized in justifying these end-products, rather
than characterizing how they are discovered or arise in the mind/brain of the
thinker.\footnote{See the introduction to \cite{reichenbach47} for some relevant
discussion. The classical work dealing mainly with the distinction between
human thought and inference on one hand and formal logic on the other is
\cite{johnsonlaird83}}
\item This distinction between discovery and justification starts to apply
already in mathematics before even approaching 	``everyday'' thinking. Theorems are
\textit{discovered} via largely intuitive (read as ``not scientifically explained
yet'') means, they are \textit{justified} (checked for validity) by formal tools
of logic.\footnote{Although the book itself is advanced for introductory level,
\cite[\S 16]{quine40} discusses this issue.}

\end{itemize}

\section{Symbols}

\begin{itemize}
\item Signs and signification are central concepts in language, logic, and
computation.

\item An initial and rough conception of signification is a three-part
relation:\footnote{If you are interested in the study of signs, a good place
to start reading is the founders of the science of signs -- named
\uterm{semiotics} or \uterm{semiology}: Ferdinand de Saussure's ``Course
in General Linguistics'' and Charles Sanders Peirce' (pronounced like the
word \intx{purse}) ``Collected Papers''. Be warned that Peirce might be
quite challenging for beginners, but Saussure is a must read for everyone with a
serious interest in language.}

\Tree{ & \K{\textit{user}}\TRi[-3] &\\
\K{\textit{sign}}  & & \K{\textit{object}}
}

\item Again some initial and rough characterizations: 
	\begin{itemize}
	\item A user uses a sign to \uterm{refer} to an object.
 	\item A sign \uterm{denotes} an object.
	\item A user has certain \uterm{intentions} about an object. 
	\end{itemize}
\end{itemize}


\section{Propositions}

\begin{itemize}
\item Let's characterize what a \uterm{proposition} is, indirectly  by way of 
natural language. Our characterization has two parts:
\begin{itemize}
\item[i.] Any expression that you can insert into the contexts ``\cntx\ is true'' or
``\cntx\ is false'' \textit{denotes} a proposition.\footnote{The contexts ``It is
the case that \cntx'' or ``It is not the case that \cntx'' will equally do,
barring some complications due to quotation. It gets yet more complicated when it comes to Turkish.
Can you see why?}
\item [ii.] There are no propositions that cannot be denoted as such.
\end{itemize}
	
The following expressions denote propositions:
	\begin{itemize}
	\item[]The earth revolves around the sun.
	\item[]D\"unya G\"une\c s etraf\i{}nda d\"on\"uyor.
	\item[]John likes Mary.
	\item[] If you multiply an even number with an odd number, you obtain an
	even number.
	\end{itemize}

while the following do not:
	\begin{itemize}
	\item[]Around the sun
	\item[]D\"unya G\"une\c s etraf\i{}nda d\"on\"uyor mu?
	\item[]Because John likes Mary
	\item[] If you multiply an even number with an odd number
	\end{itemize}

\item In other words, \uterm{declarative sentences} denote propositions.

\item Now let's assume that every proposition whatsoever has at least one
declarative sentence to express it.

\item From all we have above it follows that:

		Propositions are objects that can be (said to be) true or false.

\item Another way of saying this is that propositions are objects that have
\uterm{truth values}. 

\item This much explication of what a proposition is will suffice for our
purposes.

\item Propositional logic is called so, because its \uterm{atomic}\footnote{The
adjective \intx{atomic} serves to suggest that atomic symbols  cannot be broken
down to further components. This use of \intx{atomic} is a remnant from a
pre-nuclear conception of atoms.} symbols refer to objects called
\uterm{propositions}.

\item Can you see what could be the problem with  equating declarative sentences
with propositions?\footnote{Note to be read after discussing the question. For
Willard van Orman Quine, one of the giants of analytical philosophy, what is
problematic is not equating propositions with their linguistic expressions but
rather the opposite. He objects to  maintaining that there are abstract objects
called propositions which have their own life independent of the means that
express or denote them. Here I chose the ``anti-Quinean'' exposition, because
the discussion of Quine's objection is rather advanced for the current stage.
See his ``Two dogmas of empiricism'' for a starter on this arguably the most
central issue of analytical philosophy.}


\item We said that expressions of propositional logic denote propositions, but
what are those expressions?

\end{itemize}

\section{Defining a language $L_0$}
\label{induc}

\ezimeti{
\item Any specification of a language starts with its alphabet.
\item[] Our alphabet for $L_0$ -- the name we will give to our language -- is made up of three sets:

\item[] Basic symbols:

\begin{align}
	P = \crbr{p,q,r,s,p_1,q_1,r_1,s_1,\ldots}
\end{align} 

\item[] Connective symbols:
 
\begin{align}
	C_1 &= \crbr{-} \\
 	C_2 &= \crbr{\land,\lor}
\end{align} 

\item[] Parentheses: \sysm{\crbr{(,)}}

\item We can also collect the parts of our alphabet under a single set:

\begin{align}
\Sigma = P \cup C_1 \cup C_2 \cup \crbr{(,)}
\end{align} 

\item Let's call an \uterm{expression}, any finite sequence of (possibly
repeated) elements from $\Sigma$. The following are some example expressions:

\[
p(\land r_{4291841}\lor-\quad\quad p-p \quad\quad   -))  \quad\quad \land\lor\land \quad\quad q_{345}\land
\]

\item It is not hard to see that there is no bound to the expressions we can
form this way. Call this non-finite set of all the possible finite expressions
formed by putting together a selection of symbols from $\Sigma$ in a specific
order $\Sigma^*$.

\item Usually, we are interested in expressions fulfilling certain
criteria -- a subset of the set of all possible expressions. We distinguish
these special expressions as the grammatical sentences of the language we are
interested in.  

\item Now we can define the grammatical expressions (sentences of \uterm{well-formed formulas})
of our language \sysm{L_0}. And we will see
that $L_0 \subset \sigmastar$.

\item As you might have already realized, we identify a language with the set of its grammatical sentences. 

\hrulefill
\begin{udefinition}{Well-formed formulas of \sysm{L_0}.}
\ezimeti{
\item[i.] \sysm{\alpha \in L_0}, if \sysm{\alpha \in P}.
\item[ii.] if $\alpha \in L_0$, so is $(\gamma\alpha)$, for $\gamma\in C_1$.
\item[iii.] if $\alpha$, $\beta \in L_0$, then so is $(\alpha\gamma\beta)$, for
$\gamma \in C_2$.
\item[iv.] Nothing else is in $L_0$.
}
\end{udefinition}
\hrulefill

\item An analytic procedure for defining the notion ``sentence of $L_0$''. We
start from a complex expression and go down.

\hrulefill
\begin{udefinition}[wff's of $L_0$, ``top-down'']
\ezimeti{
\item[]
\item[i.]
$\alpha$ is a wff if $\alpha\in P$.
\item[ii.]  
$(-\alpha)$ is a wff iff $\alpha$ is a wff. 
\item[iii.]
$(\alpha\land\beta)$ is a wff iff $\alpha$ and $\beta$ are wff's. 
\item[iv.] 
$(\alpha\lor\beta)$ is a wff iff $\alpha$ and $\beta$ are wff's.
\item[v.]
Any expression that falls out of the above is not a wff.
}
\end{udefinition}
\hrulefill



\item Here is a synthetic way of specifying our sentences. We start with the
simplest expressions and go up.

\hrulefill
\begin{udefinition}[$L_0$, ``bottom-up'']\label{defbot}
An expression is a wff (or belong to $L_0$)
iff it is built from the
elements of $P$ by applying a finite number of the following
operations:\footnote{What is the difference between the parentheses on the left
and right side of the equalities?}
\begin{align}
f_-(\alpha) & = (-\alpha)\\
f_\land(\alpha,\beta) & = (\alpha\land\beta)\\
f_\lor(\alpha,\beta) & = (\alpha\lor\beta)
\end{align}
\end{udefinition}
\hrulefill


\item This could be put more formally:

 \hrulefill
 \begin{udefinition}[$L_0$, ``bottom-up'']
 An expression $\alpha$ is a wff iff there exists a sequence of expressions
 ordered in increasing length:

\[
 \alpha_1,\alpha_2,\ldots,\alpha_n
\]

such that \sysm{\alpha = \alpha_n}, and for any $\alpha_i$ for $i\leq n$, either
\ezimeti{
\item[i.] $\alpha_i \in P$;
\item[ii.] or there exists $j,k < i$ such that
$\alpha_i=f_\land(\alpha_j,\alpha_k)$ or $\alpha_i=f_\lor(\alpha_j,\alpha_k)$
\item[iii.] or there exists a $j < i$ such that $\alpha_i=f_-(\alpha_j)$,

where $f_-$, $f_\land$, and $f_\lor$ are as defined in
Definition~\ref{defbot}.
}
 \end{udefinition}
 \hrulefill

\item All these definitions not only define what the wff's of $L_0$ are, but
also their structure.
\item This is best observed over \uterm{derivation} (or construction) trees.

\begin{uexample}[Derivation trees]
Draw the derivation tree of \sysm{((p\land q)\lor((- r)\lor (q\land(- s))))}



\end{uexample}

}

\section{Conjunction, alternation, negation}
\ezimeti{

\item In the previous section, we established the first half of a formal system. We know which
sequences of symbols constitute a sentence of our system. However, we do not yet
know what our sentences mean. Let's start with the simplest expressions of
$L_0$.

\item We will take our basic symbols to refer to propositions. For instance, we
can agree on things like, $p$ stands for the proposition that the snow is white.
Given this, we can see that $p$ is interchangeable with declarative sentences
that express this proposition. Therefore, we can say 

\item[] $p$ is interchangeable with `The snow is white.'
\item[] $p$ is interchangeable with `Kar beyazd\i{r}.' 
\item[] and, so on.

\item Sometimes you will see `means', `abbreviates', `stands for', `equals to', `=', and so on, in place of
`is interchangeable with'.

\item When we use a basic symbol of $L_0$ as part of a sentence of $L_0$, which
possibly consists of only that symbol, we assert that the proposition referred
to by the symbol holds. So, 
\begin{align}\label{p}
p
\end{align}
says that the snow is white.

\item The way to assert that a proposition named by a symbol does not hold, or,
equivalently, to deny that a proposition named by a symbol, one puts a negation
sign in front of it . For instance,

\begin{align}
(-p)
\end{align}
says that it is not the case that the snow is white.

\item In science and other discourses, we are not only interested in
what propositions are expressed, but also whether the expressed propositions actually hold or not.

\item For any proposition whether it holds or not is indicated by its
\uterm{truth value}.\footnote{Note that we are counting on our intuitions here
for the meaning of ``holding'' \versus\ ``not holding''. A mathematical
rendition of this concept will follow soon.} If a proposition holds, we will say
that it is true, and its truth value is 1. If a proposition does not hold, we
will say that it is false, and its truth value is 0. Using the first two natural
numbers to designate truth values is totally arbitrary, you can pick any two symbols which will not lead
to confusion.

\item We will assume that, every proposition expressible in our system is either
true or false, there is no case in between.

\item A sentence consisting only of a basic symbol of $L_0$ has the same truth
value as the proposition it expresses. Therefore, \xref{p} is true, or has the
truth value 1, if the snow is actually white, and is false, or has the truth
value 0, if it is not the case that snow is white. 

\item From now on we will directly speak of the truth or falsity of the
sentences of $L_0$, as well as of the propositions that they refer to. 

\item The negation of a sentence consisting only of a basic symbol has the opposite truth value of the symbol.
`-p' is true if and only if `p' is false.

\item Of course, it would not be much interesting to speak about propositions
and their denial only one-by-one. 

\item One way to form complex sentences out of simple ones is
\uterm{conjunction}. On the meaning side, a conjunction asserts that both
conjuncts are true. If at least one of the conjuncts fails to be true, then the
conjunction fails to be true as well. If $p$ stands for snow's being white and
$q$ stands for Berlin being the capital of France, 
\begin{align}
(p \land q)
\end{align}
is true if and only if the snow is white and Berlin is the capital of France.

\item Another binary connective is \uterm{alternation} (or
\uterm{disjunction}).
It differs from conjunction in being more tolerant about falsehood. The
conjunction of two sentences come out true if at least one of the alternates is
true.

\begin{align}
(p \lor q)
\end{align}

is true if and only if the snow is white, or Berlin is the capital of France, or
both.

\item Note that we use ``if and only if'' in stating the cases under which
conjunction and alternation are true. This is to say that they are false
otherwise. 

\item We would still have a quite impoverished language, if our connectives were
to apply only to basic symbols. Fortunately, our connectives are totally blind
to the internal makeup of the sentences they connect.\footnote{With some abuse
of the meaning of ``connect'', we treat negation as a connective as well, a
one-place connective.} All they care, as our definition of sentence given in the
previous section makes clear, is that they connect sentences (or
\uterm{formulas}) of propositional logic, either simple or complex.

\item We will use capital symbols $P,Q,R,S$ and their subscripted forms as
variables standing for formulas.


\item For any formulas $P$, $Q \in L_0$:
\begin{align*}
(P \land Q) &\text{ is true, if and only if both } P \text{ and } Q \text{ are 
true.}\\
(P \lor Q) &\text{ is true, if and only if at least one of } P \text{ and } Q \text{
is true.}\\
(-P) &\text{ is true, if and only if } P \text{ is false.}
\end{align*}

\item Conjunction and alternation enjoy certain properties familiar from set
union and intersection:
\begin{align*}
(P\land P) &= P\\
(P\land Q) &= (Q\land P)\\
((P\land Q)\land R) &= (P\land (Q\land R))
\end{align*}
and likewise for alternation.
\item Observe that the equalities we list above are on the basis of truth
values. There is no way the two equated sentences can differ in their truth
value.

\item Our definition of $L_0$ requires the insertion of parenthesis in every step
of connection, thereby making every possible sentence ambiguous with regards to
what is grouped with what. However, for ease of inspection and writing, we will
omit parentheses where no ambiguity arises. 

\item[] For instance there is no harm in writing `\sysm{P\land Q}' instead of
`\sysm{(P\land Q)}', or `\sysm{P\land Q\land R}' instead of `\sysm{((P\land Q)\land
R)}'.

\item But we need to agree on a convention when omitting parentheses from
sentences with negation. For instance if we have `\sysm{-P \land Q}', we need to
make clear the intended scope of negation, do we mean `\sysm{(-(P\land Q))}' or
`\sysm{((-P)\land Q)}'? We will say that negation \uterm{binds more tightly}
than conjunction and alternation. In this convention, `\sysm{-P \land Q}' will mean
`\sysm{((-P)\land Q)}'. In simplifying `\sysm{(-(P\land Q))}', the furthest we
can go is  `\sysm{-(P\land Q)}'

\hrulefill
\begin{uexample}
Let $p$ abbreviate `John took vitamin C', and $q$, `John got flu.'

\begin{tabular}{rl}
$p \land q$ & John took vitamin C and John got flu.\\
$-p \land q$ & John did not take vitamin C and John got flu.\\
$p \land -q$ & John took vitamin C and John did not get flu.\\
$-p \land -q$& John did not take vitamin C and John did not get flu
\end{tabular}

What would `$-(p\land q)$' or `$-(p \lor q)$' be? 
\end{uexample}
\hrulefill

}

\section{Truth functions, truth tables, valuations}
\ezimeti{
\item  As our definitions for conjunction, disjunction and negation reveal, the
truth value of a formula formed by a certain connective depends entirely on the truth
values of the formula(s) being connected and the definition of the connective. 

\item For instance you can see conjunction as an $f: \{0,1\} \times \{0,1\}
\rightarrow \{0,1\}$. Likewise for alternation. Negation is an $f: \{0,1\}
\rightarrow \{0,1\}$, namely $\{(0,1),(1,0)\}$.

\item A more pictorial way of representing this is to use a \uterm{truth table}.

\item[] For conjunction:

$$
\begin{array}{ccc}
P & Q & P\land Q \\ \hline
1 & 1 & 1\\
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 0
\end{array}
$$

\item This property makes our connectives \uterm{truth functional}.

\item Can you think of a non-truth functional connective from natural language,
which forms a declarative sentence by connecting two
declarative sentences, but the truth value of the resulting sentence does not
depend only on the truth values of the connected sentences.

\item A \uterm{valuation} is a function from the set of propositional symbols to
$\{ 0,1\}$. It tells which of our basic propositions are true and which are
false.

\item[] From this perspective, every formula imposes a filter on valuations; it
picks a subset of the set of all possible valuations. This subset is the set of
valuations that make the formula in question true.

}

\section{Conditional}
\ezimeti{
\item Under what circumstances would you consider the person who uttered the
following sentence to have kept her word?

\item[] ``If I get a job next summer, then I will marry you.''


\item We add a new connective `$\imp$', named \uterm{conditional} (or
\uterm{material conditional}), and which is read ``if\ldots then'', or ``only
if''. It has the following truth-table: 

$$
\begin{array}{ccc}
P & Q & P\imp Q \\ \hline
1 & 1 & 1\\
1 & 0 & 0\\
0 & 1 & 1\\
0 & 0 & 1
\end{array}
$$


\item The conditional is redundant, you can define it in terms of the other
connectives.

\item A related connective, which is again redundant, is `$\bicond$', named
\uterm{biconditional}. It is read ``if and only if'' and comes out true whenever
the two components agree in their truth value.

\item Both connectives of this section bind less tightly then conjunction and
alternation. Therefore `$p\lor q\imp r$' is `$((p\lor q) \imp r)$'; you can
simplify `$(p\lor(q\imp r))$' at most as `$p\lor(q\imp r)$'.
 


}

\section{Validity, contradictoriness, consistency}

\ezimeti{
\item A formula $P$ is \uterm{valid} if and only if it always comes out true
regardless of the truth values of its components. 
\item[] We designate the validity of $P$ by:

$$
\models P
$$

\item[] Valid formulas of propositional logic are also called
\uterm{tautologous}. 

\item[] Some examples:

\begin{align*}
&\models p\lor -p \\ &\models p\imp(q\imp p) 
\end{align*}


\item A formula $P$ is \uterm{contradictory}, designated $\not\models P$, if and only if it always comes out
false regardless of the truth values of its components. 

\hrulefill
\begin{uexercise}
Can you think of a general method of testing whether a given formula is valid,
contradictory, or consistent?
\end{uexercise}
\hrulefill
}

\section{Implication, equivalence and substitution}
\ezimeti{

\item A formula $P$ \uterm{implies} $Q$, designated $P\models Q$, if and only if
$\models P\imp Q$.


\item Two formulas $P$ and $Q$ are \uterm{equivalent}, designated $P\equiv Q$, if and only if
$\models P\leftrightarrow Q$.

\item Given a formula $R$, $R \equiv R_{[P/Q]}$ iff
$P\equiv Q$, where $R_{[P/Q]}$ means the formula formed by substituting $P$ for
each occurrence of $Q$ in $R$. 

\hrulefill
\begin{uexercise}
Given `$p \imp q$', state which of the following imply or are implied by it:
\setlength{\arraycolsep}{1.4em}
$$
\begin{array}{cccc}
-p & q & -p \lor q & q \land r  \\
&  p \imp q \lor r &  p \lor r \imp q  & \\   
p \imp q \land r  & (p\imp q) \lor r &  -q\imp -p &  p \land r \imp q \\
\end{array}
$$
\end{uexercise}
\hrulefill
}

\section{Natural deduction}

Natural deduction is a method for \uterm{deriving} a formula $Q$ from (possibly
empty) set of formulas $\crbr{P_1,P_2,\ldots,P_n}$, called \uterm{premisses},
such that $Q$ is true in case all the premisses are jointly true. The relation
of derivability is designated as:


$$P_1,P_2,\ldots,P_n \vdash Q$$

and,

$$P_1,P_2,\ldots,P_n \vdash Q\quad \text{ iff }\quad P_1\land P_2\land \ldots\land P_n
\models Q$$

The method of natural deduction  involves a set of \uterm{inference rules} and
\uterm{proof techniques}.


\subsection{Inference rules}

\begin{itemize}
\item[]{ Simplification:}
$$
\begin{array}{c}
P\land Q\\ \hline
P
\end{array}
\quad
\quad
\quad
\begin{array}{c}
P\land Q\\ \hline
Q
\end{array}
$$

\item[]{Adjunction:}

$$
\begin{array}{c}
P \\
Q \\ \hline
P \land Q
\end{array}
\quad
\quad
\quad
\begin{array}{c}
P \\
Q \\ \hline
Q \land P
\end{array}
$$

\item[]{Addition:}

$$
\begin{array}{c}
P \\ \hline

P \lor Q \\
\end{array}
\quad
\quad
\quad
\begin{array}{c}
Q \\ \hline
P \lor Q
\end{array}
$$

\item[]{Modus ponens (MP):}

$$
\begin{array}{l}
P \imp Q \\ 
P \\ \hline
Q \\ 
\end{array}
$$

\item[]{Modus tollens (MT):}

$$
\begin{array}{l}
P \imp Q \\ 
-Q \\ \hline
-P \\ 
\end{array}
$$
\item[]{Modus tollendo ponens (MTP):}

$$
\begin{array}{l}
P \lor Q \\
-P \\ \hline
Q \\
\end{array}
\quad
\quad
\quad
\begin{array}{l}
P \lor Q \\
-Q \\ \hline
P \\
\end{array}
$$

\item[]{Double negation:}

$$
\begin{array}{c}
--P \\ \hline 
P \\ 
\end{array}
\quad
\quad
\quad
\begin{array}{c}
P \\ \hline 
--P \\ 
\end{array}
$$

\item[]{Repetition:}

$$
\begin{array}{c}
P \\ \hline
P \\ 
\end{array}
$$
\end{itemize}



\subsection{Proof techniques}

\subsubsection{Direct proof}

Let's illustrate how a direct proof works over an example,

$$p\land q \vdash q\land p$$

We start by designating our target -- the formula we aim to derive:


In proofs we can pick and add any of our premisses at any point, if we
believe it will be useful. Here we do that, and add our only premiss.

\begin{logicproof}{2}
	\sform{q\land p} & \\
	\form{p\land q} &\label{prem} Prem.
\end{logicproof}

Next we observe that we can apply one of our rules, simplification, to the premiss twice,
obtaining $p$ and $q$. In the ideal case we write the justification of each
step. 

\begin{logicproof}{2}
	\sform{q\land p} & \\
	\form{p\land q} & Prem.\\		
	\form{p} & \ref{prem} Simp.\\		
	\form{q} & \ref{prem} SSimp.
\end{logicproof}


Next we use another rule, adjunction, to form the desired formula

\begin{logicproof}{2}
	\sform{q\land p} & \\
	\form{p\land q} & Prem.\\		
	\form{p} & \ref{prem} Simp.\\		
	\form{q} & \ref{prem} Simp.\\		
	\form{q\land p} & Adj.
\end{logicproof}


In a direct proof, when we obtain the formula we wanted to derive, we
``box'' the proof, and cancel the initial $Show$.

\begin{logicproof}{3}
	\cform{q\land p} & \\
	\begin{subproof}
	\form{p\land q} & Prem.\\		
	\form{p} & \ref{prem}\label{s1} Simp.\\		
	\form{q} & \ref{prem}\label{s2}  Simp.\\		
	\form{q\land p} & \ref{s1}, \ref{s2} Adj.
	\end{subproof}
\end{logicproof}

Although direct proof is conceptually simple, it is seldom adequate on
its own.


\subsubsection{Conditional proof}

When the target formula is a conditional, we assume the antecdent and
show that the consequent is derviable under this assumption. Take,  

\begin{align}
-q\imp -r,\quad p \imp r\;\vdash\; p\imp q
\end{align}




Again we start with a \textit{Show} line. 

\begin{logicproof}{3}
	\sform{p\imp q} &
\end{logicproof}

In a conditional proof, we start with assuming the antecedent, $p$ in
this case:

\begin{logicproof}{3}
	\sform{p\imp q} & \\
	\form{p} & \label{bass}Asmp.		
\end{logicproof}

The aim is to derive the consequent $q$. In this task, in addition to
the premisses, we are allowed to make use of the assumption $p$.From here on we
proceed as in a direct proof of $q$, namely applying the available rules to the
formulas available. It is crucial to observe that we cannot apply MP to $p\imp
q$ and $p$. Any formula that has an uncancelled \textit{Show} is UNavailable in a
proof. In order to proceed,  we take a premiss that we can feed into the MP rule
together with $p$ and obtain $r$.


\begin{logicproof}{3}
	\sform{p\imp q} & \\
	\form{p} & \label{bass}Asmp.	\\	
	\form{p\imp r} & \label{bp2} Prem.\\
	\form{r} & \ref{bass}, \ref{bp2} \label{br}  MP
\end{logicproof}

The rest of the proof proceeds in a similar fashion, eventually
obtaining $q$, boxing the proof and cancelling the \textit{Show}.

\begin{logicproof}{3}
	\cform{p\imp q} & \\
	\begin{subproof}
	\form{p} & \label{bass}Asmp.	\\	
	\form{p\imp r} & \label{bp2} Prem.\\		
	\form{r} & \ref{bass}, \ref{bp2} \label{br}  MP\\
	\form{--r} & \ref{br}\label{b-r} DN\\
	\form{-q\imp -r} & \label{bp1} Prem.\\
	\form{--q} & \ref{b-r}, \ref{bp1}\label{b--q} MT\\
	\form{q} & \ref{b--q} DN
	\end{subproof}
\end{logicproof}

Now we turn to an example that calls for nested \textit{Show}s. Take,

\begin{align}
p\imp(q\imp r),\quad p\imp(r\imp s)\; \vdash\; p\imp(q\imp s)
\end{align}

We start our conditional proof by assuming $p$:

\begin{logicproof}{3}
	\sform{p\imp (q\imp s)} & \\
	\form{p} & \label{cass1}Asmp.
\end{logicproof}

At this point we have a new goal, proving $q\imp s$. If we can do that,
then we can conclude that assuming $p$ yields $q\imp s$, achieving our initial
goal.

\begin{logicproof}{3}
	\cform{p\imp (q\imp s)} & \\
	\form{p} & \label{cass1}Asmp.	\\	
	\sform{(q\imp s)} & \\
\end{logicproof}

In our \uterm{subproof} we proceed as in a conditional proof, assuming
$q$ and trying to obtain $s$: 


\begin{logicproof}{3}
	\sform{p\imp (q\imp s)} & \\
	\form{p} & \label{cass1}Asmp.	\\	
	\sform{(q\imp s)} & \\
	\form{q} & \label{cass2}Asmp.	\\
\end{logicproof}

Once we obtain $s$ we box its proof and cancel the \textit{Show}
preceeding our interim goal $q\imp s$, 

\begin{logicproof}{3}
	\sform{p\imp (q\imp s)} & \\
	\form{p} & \label{cass1}Asmp.	\\	
	\cform{(q\imp s)} & \\
	\begin{subproof}	
	\form{q} & \label{cass2}Asmp.	\\
	\form{p\imp(q\imp r)} & Prem. \label{cp1}\\ 
	\form{q\imp r} & \ref{cass1}, \ref{cp1} MP\label{cqr}\\
	\form{r} & \ref{cass2}, \ref{cqr} MP\label{cr}\\
	\form{p\imp (r\imp s)} & \label{cp2} Prem.\\		
	\form{r\imp s} & \ref{cass1}, \ref{cp2}  MP\label{crs}\\		
	\form{s} & \ref{cr}, \ref{crs}  MP		
	\end{subproof}
\end{logicproof}

Our initial aim was to see whether we can obtain $q\imp s$ under the
assumption $p$ (and of course our premisses). Any formula that is preceeded by a
cancelled \cform is available for use in the unboxed parts of our proof. Given
that, we see that we derived $q\imp s$ on the assumption $p$. We box the proof
and cancel our top-most \emph{Show}:

\begin{logicproof}{3}
	\cform{p\imp (q\imp s)} & \\
	\begin{subproof}
	\form{p} & \label{cass1}Asmp.	\\	
	\cform{(q\imp s)} & \\
	\begin{subproof}	
	\form{q} & \label{cass2}Asmp.	\\
	\form{p\imp(q\imp r)} & Prem. \label{cp1}\\ 
	\form{q\imp r} & \ref{cass1}, \ref{cp1} MP\label{cqr}\\
	\form{r} & \ref{cass2}, \ref{cqr} MP\label{cr}\\
	\form{p\imp (r\imp s)} & \label{cp2} Prem.\\		
	\form{r\imp s} & \ref{cass1}, \ref{cp2}  MP\label{crs}\\		
	\form{s} & \ref{cr}, \ref{crs}  MP		
	\end{subproof}
	\end{subproof}
\end{logicproof}

\subsubsection{Indirect proof}

An indirect proof starts with assuming the opposite (denial) of what is being
tried to be proved. If this assumption leads to a contradiction -- having both
$P$ and $-P$ for some formula $P$, we conclude that the formula we denied in the
beginning holds. This technique is sometimes called `proof by contradiction' or
\emph{reductio ad absurdum}.

\begin{uexample}
Prove \ref{ind1} by natural deduction.
\begin{align}\label{ind1}
-p\imp q,\quad p\imp q\; \vdash\; q
\end{align}

We start by denying our target $q$:

\begin{logicproof}{2}
\sform{q} &\\
\form{-q} & Asmp. \label{eass}
\end{logicproof}

From here on our aim is to derive a contradiction. \emph{Any} formula $P$ such
that we have both $P$ and $-P$. We achieve this aim as follows:

\begin{logicproof}{2}
\sform{q} &\\
\form{-q} & Asmp. \label{eass}\\
\form{p\imp q} & Prem.\label{ep1}\\
\form{-p} & \ref{eass}, \ref{ep1} TP\label{e-p}\\
\form{-p\imp q} & Prem.\label{ep2}\\ 
\form{q} & \ref{e-p}, \ref{ep2} MP
\end{logicproof}

Our assumption $-q$ has allowed us to derive $q$, resulting in a contradiction.
We can now conclude that the assumption $-q$ is unatainable, and therefore $q$
must hold. This completes the proof. We box and cancel as usual. 

\begin{logicproof}{2}
\cform{q} &\\
\begin{subproof}
\form{-q} & Asmp. \label{eass}\\
\form{p\imp q} & Prem.\label{ep1}\\
\form{-p} & \ref{eass}, \ref{ep1} MP\label{e-p}\\
\form{-p\imp q} & Prem.\label{ep2}\\ 
\form{q} & \ref{e-p}, \ref{ep2} MP
\end{subproof}
\end{logicproof}

\qed
\end{uexample}


\begin{uexample}
Take the following argument:

\begin{quote}

Harry is the murderer, only if he was at the apartment around 10pm. The police will
find a fingerprint, provided that he was at the apartment around 10pm. It
is not the case that if he forgot to wear a glove, the police will find a
fingerprint. Therefore, Harry is not the murderer.

\end{quote}

Given the symbolization,

\begin{itemize}
\item[] $m$: Harry is the murderer;
\item[] $r$: Harry was at the apartment around 10pm;
\item[] $p$: The police will find a fingerprint;
\item[] $t$: Harry forgot to wear a glove, 
\end{itemize}

the argument will be:\footnote{Note that `$p$ only if $q$' is $p\imp q$, while
`$p$ provided that $q$' is $q\imp p$. }

\begin{align}
m\imp r,\quad r\imp p\quad -(t\imp p)\;\vdash\; -m
\end{align}


We proceed with an indirect proof:

\begin{logicproof}{2}
\sform{-m} & \\
\form{--m} & Asmp.\label{dass}\\
\form{m}  & \ref{dass} DN\label{dm}
\end{logicproof}

The same proof could be started as,

\begin{logicproof}{2}
\sform{-m} & \\
\form{m}  & Asmp.\label{dass}\label{dm}
\end{logicproof}
leaving the application of DN implicit. Now, the aim is to derive a
contradiction. The most basic strategy is to derive some formula that
contradicts what we already have in the proof and unused premisses, if there are
any. Let's attempt to derive $t\imp p$ via a conditional subproof:  

\begin{logicproof}{2}
\sform{-m} & \\
\form{m}  & Asmp.\label{dm}\\
\cform{t\imp p} &\\
\begin{subproof}
\form{t} & \label{dass}\\
\form{m\imp r} & Prem.\label{dp1}\\
\form{r} & \ref{dm}, \ref{dp1} MP\label{dr}\\
\form{r\imp p} & Prem.\label{dp2}\\
\form{p} & \ref{dr}, \ref{dp2} MP\label{dr}
\end{subproof}
\end{logicproof}

Having proved $t\imp p$, we arrived at a contradiction, namely with the premiss
$-(t\imp p)$:

\begin{logicproof}{2}
\sform{-m} & \\
\form{m}  & Asmp.\label{dm}\\
\form{-(t\imp p)} & Prem.\label{dp3}\\
\cform{t\imp p} &\\
\begin{subproof}
\form{t} & \label{dass}\\
\form{m\imp r} & Prem.\label{dp1}\\
\form{r} & \ref{dm}, \ref{dp1} MP\label{dr}\\
\form{r\imp p} & Prem.\label{dp2}\\
\form{p} & \ref{dr}, \ref{dp2} MP\label{dr}
\end{subproof}
\end{logicproof}
which completes our proof as:

\begin{logicproof}{2}
\cform{-m} & \\
\begin{subproof}
\form{m}  & Asmp.\label{dm}\\
\form{-(t\imp p)} & Prem.\label{dp3}\\
\cform{t\imp p} &\\
\begin{subproof}
\form{t} & \label{dass}\\
\form{m\imp r} & Prem.\label{dp1}\\
\form{r} & \ref{dm}, \ref{dp1} MP\label{dr}\\
\form{r\imp p} & Prem.\label{dp2}\\
\form{p} & \ref{dr}, \ref{dp2} MP\label{dr}
\end{subproof}
\end{subproof}
\end{logicproof}


\qed
\end{uexample}




\section*{Self study} Except natural deduction, you can review what we have
covered in propositional logic from \cite{parteeetal90}, Chapter 6, up to
Section 6.5. Please note some terminological differences: they use ``statement
logic'' for ``propositional logic'', ``logical consequence'' for
``implication'', ``disjunction'' for ``alternation'', ``contingent'' for
``consistent'', and so on.

Please do not try to memorize Table 6.2. The caption of the table, ``Laws of
statement logic'' is misleading. These are not laws, they are just valid
formulas, which are on an equal status with any other valid formula of propositional
logic. Try to see and work out why they are valid, and that's it for now.

As for the inductive definitions of the well-formed expressions of $L_0$ and
their top-down analysis and bottom-up generation (Section \ref{induc} of the
notes), they can be reviewed from
the notes. If you do not fully understand them at the moment, don't worry. Give
priority to the logic part. 


\renewcommand{\bibsep}{0pt}
\renewcommand{\bibfont}{\small}
\bibliography{ozge}
\bibliographystyle{natgig}
\end{document}




\section{Structure and Grouping}

\section{Use versus mention}

\section{Validity}

TODO: On the language part, start by all expressions, then expressions that are
formulas, then theorems.


Exercise: tree drawing for grouping.
Exercise: make up an exercise using Quine's `neither nor'
Exercise: N1 p80 q 9.9, abbreviation exercise. also 9.17
